library(caTools) # Instalar con install.packages('caTools')
set.seed(123) # Para que salgan los mismos resultados que en el curso
split = sample.split(dataset$Purchased, SplitRatio = 0.75)
training_set = subset(dataset, split == TRUE)
testing_set = subset(dataset, split == FALSE)
# Escalado de valores
# Hay que indicar las columnas donde hacer el escalado
training_set[, 1:2] = scale(training_set[, 1:2])
testing_set[, 1:2] = scale(testing_set[, 1:2])
# Ajustar el K-NN con el conjunto de entrenamiento
# y hacer las predicciones con el conjunto de testing
library(class) # Instalar con install.packages('class')
y_pred = knn(train = training_set[, -3],
test = testing_set[, -3],
cl = training_set[, 3],
k = 5)
# Crear la matriz de confusion
cm = table(testing_set[, 3], y_pred)
# Visualizacion del conjunto de Entrenamiento
# Con install.packages('ElemStatLearn') salta error
# Entrar en https://cran.r-project.org/src/contrib/Archive/ElemStatLearn/
# Descargar e instalar version ElemStatLearn_2015.6.26.2.tar.gz
library(ElemStatLearn)
set = training_set
X1 = seq(min(set[, 1]) - 1, max(set[, 1]) + 1, by = 0.01)
X2 = seq(min(set[, 2]) - 1, max(set[, 2]) + 1, by = 0.01)
grid_set = expand.grid(X1, X2)
colnames(grid_set) = c('Age', 'EstimatedSalary')
y_grid = knn(train = training_set[, -3],
test = grid_set,
cl = training_set[, 3],
k = 5)
plot(set[, -3],
main = 'K-NN (Conjunto de Entrenamiento)',
xlab = 'Edad', ylab = 'Sueldo Estimado',
xlim = range(X1), ylim = range(X2))
contour(X1, X2, matrix(as.numeric(y_grid), length(X1), length(X2)), add = TRUE)
points(grid_set, pch = '.', col = ifelse(y_grid == 1, 'springgreen3', 'tomato'))
points(set, pch = 21, bg = ifelse(set[, 3] == 1, 'green4', 'red3'))
setwd("~/repos/machinelearning-az/datasets/Part 3 - Classification")
# Importar el dataset
dataset = read.csv('data/Social_Network_Ads.csv')
dataset = dataset[, 3:5]
# Dividir los datos en conjunto de entrenamiento y conjunto de test
library(caTools) # Instalar con install.packages('caTools')
set.seed(123) # Para que salgan los mismos resultados que en el curso
split = sample.split(dataset$Purchased, SplitRatio = 0.75)
training_set = subset(dataset, split == TRUE)
testing_set = subset(dataset, split == FALSE)
# Escalado de valores
# Hay que indicar las columnas donde hacer el escalado
training_set[, 1:2] = scale(training_set[, 1:2])
testing_set[, 1:2] = scale(testing_set[, 1:2])
# Ajustar el modelo de Regresion Logistica con el conjunto de entrenamiento
classifier = glm(formula = Purchased ~ .,
data = training_set,
family = binomial)
# Prediccion de los resultados con el conjunto de Testing
prob_pred = predict(classifier, type = 'response',
newdata = testing_set[, -3]) # Todo el dataset excluyendo la columna 3
y_pred = ifelse(prob_pred > 0.5, 1, 0)
# Crear la matriz de confusion
cm = table(testing_set[, 3], y_pred)
# Visualizacion del conjunto de Entrenamiento
# Con install.packages('ElemStatLearn') salta error
# Entrar en https://cran.r-project.org/src/contrib/Archive/ElemStatLearn/
# Descargar e instalar version ElemStatLearn_2015.6.26.2.tar.gz
library(ElemStatLearn)
set = training_set
X1 = seq(min(set[, 1]) - 1, max(set[, 1]) + 1, by = 0.01)
X2 = seq(min(set[, 2]) - 1, max(set[, 2]) + 1, by = 0.01)
grid_set = expand.grid(X1, X2)
colnames(grid_set) = c('Age', 'EstimatedSalary')
prob_set = predict(classifier, type = 'response', newdata = grid_set)
y_grid = ifelse(prob_set > 0.5, 1, 0)
plot(set[, -3],
main = 'Clasificación (Conjunto de Entrenamiento)',
xlab = 'Edad', ylab = 'Sueldo Estimado',
xlim = range(X1), ylim = range(X2))
contour(X1, X2, matrix(as.numeric(y_grid), length(X1), length(X2)), add = TRUE)
points(grid_set, pch = '.', col = ifelse(y_grid == 1, 'springgreen3', 'tomato'))
points(set, pch = 21, bg = ifelse(set[, 3] == 1, 'green4', 'red3'))
install.packages('caTools')
install.packages("caTools")
install.packages('caTools')
setwd("~/repos/machinelearning-az/datasets/Part 3 - Classification")
# Importar el dataset
dataset = read.csv('data/Social_Network_Ads.csv')
dataset = dataset[, 3:5]
# Codificar la variable de clasificacion como factor
dataset$Purchased = factor(dataset$Purchased, levels = c(0, 1))
# Dividir los datos en conjunto de entrenamiento y conjunto de test
library(caTools) # Instalar con install.packages('caTools')
set.seed(123) # Para que salgan los mismos resultados que en el curso
split = sample.split(dataset$Purchased, SplitRatio = 0.75)
training_set = subset(dataset, split == TRUE)
testing_set = subset(dataset, split == FALSE)
# Escalado de valores
# Hay que indicar las columnas donde hacer el escalado
training_set[, 1:2] = scale(training_set[, 1:2])
testing_set[, 1:2] = scale(testing_set[, 1:2])
# Ajustar el clasificador con el conjunto de entrenamiento
library(e1071) # Instalar con install.packages('e1071')
classifier = naiveBayes(x = training_set[, -3],
y = training_set$Purchased)
# Prediccion de los resultados con el conjunto de Testing
y_pred = predict(classifier, newdata = testing_set[, -3]) # Todo el dataset excluyendo la columna 3
# Crear la matriz de confusion
cm = table(testing_set[, 3], y_pred)
# Visualizacion del conjunto de Entrenamiento
# Con install.packages('ElemStatLearn') salta error
# Entrar en https://cran.r-project.org/src/contrib/Archive/ElemStatLearn/
# Descargar e instalar version ElemStatLearn_2015.6.26.2.tar.gz
library(ElemStatLearn)
set = training_set
X1 = seq(min(set[, 1]) - 1, max(set[, 1]) + 1, by = 0.01)
X2 = seq(min(set[, 2]) - 1, max(set[, 2]) + 1, by = 0.01)
grid_set = expand.grid(X1, X2)
colnames(grid_set) = c('Age', 'EstimatedSalary')
y_grid = predict(classifier, newdata = grid_set)
plot(set[, -3],
main = 'Naïve Bayes (Conjunto de Entrenamiento)',
xlab = 'Edad', ylab = 'Sueldo Estimado',
xlim = range(X1), ylim = range(X2))
contour(X1, X2, matrix(as.numeric(y_grid), length(X1), length(X2)), add = TRUE)
points(grid_set, pch = '.', col = ifelse(y_grid == 1, 'springgreen3', 'tomato'))
points(set, pch = 21, bg = ifelse(set[, 3] == 1, 'green4', 'red3'))
setwd("~/repos/machinelearning-az/datasets/Part 3 - Classification")
# Importar el dataset
dataset = read.csv('data/Social_Network_Ads.csv')
dataset = dataset[, 3:5]
# Codificar la variable de clasificacion como factor
dataset$Purchased = factor(dataset$Purchased, levels = c(0, 1))
# Dividir los datos en conjunto de entrenamiento y conjunto de test
library(caTools) # Instalar con install.packages('caTools')
set.seed(123) # Para que salgan los mismos resultados que en el curso
split = sample.split(dataset$Purchased, SplitRatio = 0.75)
training_set = subset(dataset, split == TRUE)
testing_set = subset(dataset, split == FALSE)
# Escalado de valores
# Hay que indicar las columnas donde hacer el escalado
training_set[, 1:2] = scale(training_set[, 1:2])
testing_set[, 1:2] = scale(testing_set[, 1:2])
# Ajustar el clasificador de Random Forest con el conjunto de entrenamiento
library(randomForest) # Instalar con install.packages('randomForest')
classifier = randomForest(x = training_set[, -3],
y = training_set[, 3],
ntree = 10)
# Prediccion de los resultados con el conjunto de Testing
y_pred = predict(classifier, newdata = testing_set[, -3]) # Todo el dataset excluyendo la columna 3
# Crear la matriz de confusion
cm = table(testing_set[, 3], y_pred)
# Visualizacion del conjunto de Entrenamiento
# Con install.packages('ElemStatLearn') salta error
# Entrar en https://cran.r-project.org/src/contrib/Archive/ElemStatLearn/
# Descargar e instalar version ElemStatLearn_2015.6.26.2.tar.gz
library(ElemStatLearn)
X1 = seq(min(set[, 1]) - 1, max(set[, 1]) + 1, by = 0.01)
X2 = seq(min(set[, 2]) - 1, max(set[, 2]) + 1, by = 0.01)
grid_set = expand.grid(X1, X2)
colnames(grid_set) = c('Age', 'EstimatedSalary')
y_grid = predict(classifier, newdata = grid_set)
plot(set[, -3],
main = 'Random Forest (Conjunto de Entrenamiento)',
xlab = 'Edad', ylab = 'Sueldo Estimado',
xlim = range(X1), ylim = range(X2))
contour(X1, X2, matrix(as.numeric(y_grid), length(X1), length(X2)), add = TRUE)
points(grid_set, pch = '.', col = ifelse(y_grid == 1, 'springgreen3', 'tomato'))
points(set, pch = 21, bg = ifelse(set[, 3] == 1, 'green4', 'red3'))
set = training_set
X1 = seq(min(set[, 1]) - 1, max(set[, 1]) + 1, by = 0.01)
X2 = seq(min(set[, 2]) - 1, max(set[, 2]) + 1, by = 0.01)
grid_set = expand.grid(X1, X2)
colnames(grid_set) = c('Age', 'EstimatedSalary')
y_grid = predict(classifier, newdata = grid_set)
plot(set[, -3],
main = 'Random Forest (Conjunto de Entrenamiento)',
xlab = 'Edad', ylab = 'Sueldo Estimado',
xlim = range(X1), ylim = range(X2))
contour(X1, X2, matrix(as.numeric(y_grid), length(X1), length(X2)), add = TRUE)
points(grid_set, pch = '.', col = ifelse(y_grid == 1, 'springgreen3', 'tomato'))
points(set, pch = 21, bg = ifelse(set[, 3] == 1, 'green4', 'red3'))
setwd("~/repos/machinelearning-az/datasets/Part 5 - Association Rule Learning")
# Importar los datos
dataset = read.csv('data/Market_Basket_Optimisation.csv', header = FALSE)
library(arules) # Instalar con install.packages('arules')
dataset = read.transactions('data/Market_Basket_Optimisation.csv',
sep = ',')
setwd("~/repos/machinelearning-az/datasets/Part 5 - Association Rule Learning")
# Importar los datos
library(arules) # Instalar con install.packages('arules')
dataset = read.csv('data/Market_Basket_Optimisation.csv', header = FALSE)
dataset = read.transactions('data/Market_Basket_Optimisation.csv',
sep = ',')
setwd("~/repos/machinelearning-az/datasets/Part 5 - Association Rule Learning")
# Importar los datos
library(arules) # Instalar con install.packages('arules')
dataset = read.csv('data/Market_Basket_Optimisation.csv', header = FALSE)
dataset = read.transactions('data/Market_Basket_Optimisation.csv',
sep = ',')
View(dataset)
setwd("~/repos/machinelearning-az/datasets/Part 5 - Association Rule Learning")
# Importar los datos
library(arules) # Instalar con install.packages('arules')
dataset = read.csv('data/Market_Basket_Optimisation.csv', header = FALSE)
dataset = read.transactions('data/Market_Basket_Optimisation.csv',
sep = ',', rm.duplicates = TRUE)
summary(dataset)
itemFrequencyPlot(dataset)
itemFrequencyPlot(dataset, topN = 100)
itemFrequencyPlot(dataset, topN = 10)
# Apriori
setwd("~/repos/machinelearning-az/datasets/Part 5 - Association Rule Learning")
# Importar los datos
library(arules) # Instalar con install.packages('arules')
dataset = read.csv('data/Market_Basket_Optimisation.csv', header = FALSE)
dataset = read.transactions('data/Market_Basket_Optimisation.csv',
sep = ',', rm.duplicates = TRUE)
summary(dataset)
itemFrequencyPlot(dataset, topN = 10)
3*7/7500
# Entrenar algoritmo Apriori con el dataset
rules = apriori(data = dataset,
parameter = list(support = 0.003, confidence = 0.8))
rules = apriori(data = dataset,
parameter = list(support = 0.003, confidence = 0.4))
# Visualizacion de los resultados
inspect(rules[1:10])
# Visualizacion de los resultados
inspect(sort(rules, by = 'lift')[1:10])
install.packages('arulesViz')
# libraries --------------------------------------------------------------
library(arules)
library(arulesViz)
# data -------------------------------------------------------------------
path <- "~/repos/machinelearning-az/datasets/Part 5 - Association Rule Learning"
trans <- read.transactions(
file = paste0(path, "data/Market_Basket_Optimisation.csv"),
sep = ",",
rm.duplicates = TRUE
)
# data -------------------------------------------------------------------
path <- "~/repos/machinelearning-az/datasets/Part 5 - Association Rule Learning"
trans <- read.transactions(
file = paste0(path, "/data/Market_Basket_Optimisation.csv"),
sep = ",",
rm.duplicates = TRUE
)
# apriori algoirthm ------------------------------------------------------
rules <- apriori(
data = trans,
parameter = list(support = 0.004, confidence = 0.2)
)
# visualizations ---------------------------------------------------------
plot(rules, method = "graph", engine = "htmlwidget")
# libraries --------------------------------------------------------------
library(arules)
library(arulesViz)
# data -------------------------------------------------------------------
path <- "~/repos/machinelearning-az/datasets/Part 5 - Association Rule Learning/"
trans <- read.transactions(
file = paste0(path, "data/Market_Basket_Optimisation.csv"),
sep = ",",
rm.duplicates = TRUE
)
# apriori algoirthm ------------------------------------------------------
rules <- apriori(
data = trans,
parameter = list(support = 0.004, confidence = 0.2)
)
# visualizations ---------------------------------------------------------
plot(rules, method = "graph", engine = "htmlwidget")
# Entrenar algoritmo Apriori con el dataset
rules = apriori(data = dataset,
parameter = list(support = 0.004, confidence = 0.2))
# Visualizacion de los resultados
inspect(sort(rules, by = 'lift')[1:10])
setwd("~/repos/machinelearning-az/datasets/Part 5 - Association Rule Learning")
# Importar los datos
library(arules) # Instalar con install.packages('arules')
dataset = read.csv('data/Market_Basket_Optimisation.csv', header = FALSE)
dataset = read.transactions('data/Market_Basket_Optimisation.csv',
sep = ',', rm.duplicates = TRUE)
summary(dataset)
itemFrequencyPlot(dataset, topN = 10)
# Entrenar algoritmo Apriori con el dataset
rules = apriori(data = dataset,
parameter = list(support = 0.004, confidence = 0.2))
# Visualizacion de los resultados
inspect(sort(rules, by = 'lift')[1:10])
library(arulesViz)
# visualizations ---------------------------------------------------------
plot(rules, method = "graph", engine = "htmlwidget")
detach("package:arulesViz", unload = TRUE)
s
setwd("~/repos/machinelearning-az/datasets/Part 5 - Association Rule Learning")
# Importar los datos
library(arules) # Instalar con install.packages('arules')
dataset = read.csv('data/Market_Basket_Optimisation.csv', header = FALSE)
dataset = read.transactions('data/Market_Basket_Optimisation.csv',
sep = ',', rm.duplicates = TRUE)
summary(dataset)
itemFrequencyPlot(dataset, topN = 10)
# Entrenar algoritmo Apriori con el dataset
rules = apriori(data = dataset,
parameter = list(support = 0.004, confidence = 0.2))
# Visualizacion de los resultados
inspect(sort(rules, by = 'lift')[1:10])
# Representacion grafica de las regla s de asociacion
#library(arulesViz)
plot(rules, method = "graph", engine = "htmlwidget")
# Apriori
setwd("~/repos/machinelearning-az/datasets/Part 5 - Association Rule Learning")
# Importar los datos
library(arules) # Instalar con install.packages('arules')
dataset = read.csv('data/Market_Basket_Optimisation.csv', header = FALSE)
dataset = read.transactions('data/Market_Basket_Optimisation.csv',
sep = ',', rm.duplicates = TRUE)
summary(dataset)
itemFrequencyPlot(dataset, topN = 10)
# Entrenar algoritmo Apriori con el dataset
rules = apriori(data = dataset,
parameter = list(support = 0.004, confidence = 0.2))
# Visualizacion de los resultados
inspect(sort(rules, by = 'lift')[1:10])
# Representacion grafica de las regla s de asociacion
#library(arulesViz)
plot(rules, method = "graph", engine = "htmlwidget")
# Representacion grafica de las regla s de asociacion
library(arulesViz)
plot(rules, method = "graph", engine = "htmlwidget")
dataset = read.transactions('data/Market_Basket_Optimisation.csv',
sep = ',', rm.duplicates = TRUE)
summary(dataset)
View(dataset)
itemFrequencyPlot(dataset, topN = 100)
# Entrenar algoritmo Eclat con el dataset
rules = eclat(data = dataset,
parameter = list(support = 0.004, confidence = 0.2))
# Entrenar algoritmo Eclat con el dataset
rules = eclat(data = dataset,
parameter = list(support = 0.004))
# Visualizacion de los resultados
inspect(sort(rules, by = 'lift')[1:10])
library(arulesViz)
plot(rules, method = "graph", engine = "htmlwidget")
# Entrenar algoritmo Eclat con el dataset
rules = eclat(data = dataset,
parameter = list(support = 0.004, minlen = 2))
plot(rules, method = "graph", engine = "htmlwidget")
# Visualizacion de los resultados
inspect(sort(rules, by = 'support')[1:10])
setwd("~/repos/machinelearning-az/datasets/Part 5 - Association Rule Learning")
# Importar los datos
library(arules) # Instalar con install.packages('arules')
dataset = read.csv('data/Market_Basket_Optimisation.csv', header = FALSE)
dataset = read.transactions('data/Market_Basket_Optimisation.csv',
sep = ',', rm.duplicates = TRUE)
summary(dataset)
itemFrequencyPlot(dataset, topN = 100)
# Entrenar algoritmo Apriori con el dataset
rules = apriori(data = dataset,
parameter = list(support = 0.004, confidence = 0.2))
# Visualizacion de los resultados
inspect(sort(rules, by = 'lift')[1:10])
# Representacion grafica de las reglas de asociacion
library(arulesViz)
plot(rules, method = "graph", engine = "htmlwidget")
setwd("~/repos/machinelearning-az/datasets/Part 6 - Reinforcement Learning")
setwd("~/repos/machinelearning-az/datasets/Part 6 - Reinforcement Learning")
# Importar el dataset
dataset = read.csv('data/Ads_CTR_Optimisation.csv')
View(dataset)
View(dataset)
# Importing the dataset
dataset = read.csv('data/Ads_CTR_Optimisation.csv')
# Implementing Random Selection
N = 10000
d = 10
ads_selected = integer(0)
total_reward = 0
for (n in 1:N) {
ad = sample(1:10, 1)
ads_selected = append(ads_selected, ad)
reward = dataset[n, ad]
total_reward = total_reward + reward
}
# Visualising the results
hist(ads_selected,
col = 'blue',
main = 'Histogram of ads selections',
xlab = 'Ads',
ylab = 'Number of times each ad was selected')
# Upper Confidence Bound
setwd("~/repos/machinelearning-az/datasets/Part 6 - Reinforcement Learning")
# Importar el dataset
dataset = read.csv('data/Ads_CTR_Optimisation.csv')
# Implementar UCB
d = 10
N = 10000
number_of_selections = integer(d)
sums_of_rewards = integer(d)
ads_selected = integer(0)
total_reward = 0
for(n in 1:N){
max_upper_bound = 0
ad = 0
for(i in 1:d){
if(number_of_selections[i]>0){
average_reward = sums_of_rewards[i]/number_of_selections[i]
delta_i = sqrt(3/2*log(n)/number_selections[i])
upper_bound = average_reward + delta_i
}
else{
upper_bound = 1e400
}
if(upper_bound > max_upper_bound){
max_upper_bound = upper_bound
ad = i
}
}
ads_selected = append(ads_selected, ad)
number_of_selections[ad] = number_of_selections[ad] + 1
reward = dataset[n, ad]
sums_of_rewards[ad] = sums_of_rewards[ad] + reward
total_reward = total_reward + reward
}
# Upper Confidence Bound
setwd("~/repos/machinelearning-az/datasets/Part 6 - Reinforcement Learning")
# Importar el dataset
dataset = read.csv('data/Ads_CTR_Optimisation.csv')
# Implementar UCB
d = 10
N = 10000
number_of_selections = integer(d)
sums_of_rewards = integer(d)
ads_selected = integer(0)
total_reward = 0
for(n in 1:N){
max_upper_bound = 0
ad = 0
for(i in 1:d){
if(number_of_selections[i]>0){
average_reward = sums_of_rewards[i]/number_of_selections[i]
delta_i = sqrt(3/2*log(n)/number_of_selections[i])
upper_bound = average_reward + delta_i
}
else{
upper_bound = 1e400
}
if(upper_bound > max_upper_bound){
max_upper_bound = upper_bound
ad = i
}
}
ads_selected = append(ads_selected, ad)
number_of_selections[ad] = number_of_selections[ad] + 1
reward = dataset[n, ad]
sums_of_rewards[ad] = sums_of_rewards[ad] + reward
total_reward = total_reward + reward
}
ads_selected
hist(ads_selected,
col = 'blue',
main = 'Histograma de anuncios',
xlab = 'ID del Anuncio',
ylab = 'Frecuencia de visualizacion del anuncio')
# Visualizar los resultados - Histograma
hist(ads_selected,
col = 'lightblue',
main = 'Histograma de anuncios',
xlab = 'ID del Anuncio',
ylab = 'Frecuencia absoluta del anuncio')
# Muestreo de Thompson
setwd("~/repos/machinelearning-az/datasets/Part 6 - Reinforcement Learning")
# Importar el dataset
dataset = read.csv('data/Ads_CTR_Optimisation.csv')
# Implementar el Muestreo de Thompson
d = 10
N = 10000
number_of_rewards_1 = integer(d)
number_of_rewards_0 = integer(d)
ads_selected = integer(0)
total_reward = 0
for(n in 1:N){
max_random = 0
ad = 0
for(i in 1:d){
random_beta = rbeta(n = 1,
shape1 = number_of_rewards_1[i] + 1,
shape2 = number_of_rewards_0[i] + 1)
if(random_beta > max_random){
max_random = random_beta
ad = i
}
}
ads_selected = append(ads_selected, ad)
reward = dataset[n, ad]
if(reward == 1){
number_of_rewards_1[ad] = number_of_rewards_1[ad] + 1
}
else{
number_of_rewards_0[ad] = number_of_rewards_0[ad] + 1
}
total_reward = total_reward + reward
}
# Visualizar los resultados - Histograma
hist(ads_selected,
col = 'lightblue',
main = 'Histograma de anuncios',
xlab = 'ID del Anuncio',
ylab = 'Frecuencia absoluta del anuncio')
