)
# visualizations ---------------------------------------------------------
plot(rules, method = "graph", engine = "htmlwidget")
# Entrenar algoritmo Apriori con el dataset
rules = apriori(data = dataset,
parameter = list(support = 0.004, confidence = 0.2))
# Visualizacion de los resultados
inspect(sort(rules, by = 'lift')[1:10])
setwd("~/repos/machinelearning-az/datasets/Part 5 - Association Rule Learning")
# Importar los datos
library(arules) # Instalar con install.packages('arules')
dataset = read.csv('data/Market_Basket_Optimisation.csv', header = FALSE)
dataset = read.transactions('data/Market_Basket_Optimisation.csv',
sep = ',', rm.duplicates = TRUE)
summary(dataset)
itemFrequencyPlot(dataset, topN = 10)
# Entrenar algoritmo Apriori con el dataset
rules = apriori(data = dataset,
parameter = list(support = 0.004, confidence = 0.2))
# Visualizacion de los resultados
inspect(sort(rules, by = 'lift')[1:10])
library(arulesViz)
# visualizations ---------------------------------------------------------
plot(rules, method = "graph", engine = "htmlwidget")
detach("package:arulesViz", unload = TRUE)
s
setwd("~/repos/machinelearning-az/datasets/Part 5 - Association Rule Learning")
# Importar los datos
library(arules) # Instalar con install.packages('arules')
dataset = read.csv('data/Market_Basket_Optimisation.csv', header = FALSE)
dataset = read.transactions('data/Market_Basket_Optimisation.csv',
sep = ',', rm.duplicates = TRUE)
summary(dataset)
itemFrequencyPlot(dataset, topN = 10)
# Entrenar algoritmo Apriori con el dataset
rules = apriori(data = dataset,
parameter = list(support = 0.004, confidence = 0.2))
# Visualizacion de los resultados
inspect(sort(rules, by = 'lift')[1:10])
# Representacion grafica de las regla s de asociacion
#library(arulesViz)
plot(rules, method = "graph", engine = "htmlwidget")
# Apriori
setwd("~/repos/machinelearning-az/datasets/Part 5 - Association Rule Learning")
# Importar los datos
library(arules) # Instalar con install.packages('arules')
dataset = read.csv('data/Market_Basket_Optimisation.csv', header = FALSE)
dataset = read.transactions('data/Market_Basket_Optimisation.csv',
sep = ',', rm.duplicates = TRUE)
summary(dataset)
itemFrequencyPlot(dataset, topN = 10)
# Entrenar algoritmo Apriori con el dataset
rules = apriori(data = dataset,
parameter = list(support = 0.004, confidence = 0.2))
# Visualizacion de los resultados
inspect(sort(rules, by = 'lift')[1:10])
# Representacion grafica de las regla s de asociacion
#library(arulesViz)
plot(rules, method = "graph", engine = "htmlwidget")
# Representacion grafica de las regla s de asociacion
library(arulesViz)
plot(rules, method = "graph", engine = "htmlwidget")
dataset = read.transactions('data/Market_Basket_Optimisation.csv',
sep = ',', rm.duplicates = TRUE)
summary(dataset)
View(dataset)
itemFrequencyPlot(dataset, topN = 100)
# Entrenar algoritmo Eclat con el dataset
rules = eclat(data = dataset,
parameter = list(support = 0.004, confidence = 0.2))
# Entrenar algoritmo Eclat con el dataset
rules = eclat(data = dataset,
parameter = list(support = 0.004))
# Visualizacion de los resultados
inspect(sort(rules, by = 'lift')[1:10])
library(arulesViz)
plot(rules, method = "graph", engine = "htmlwidget")
# Entrenar algoritmo Eclat con el dataset
rules = eclat(data = dataset,
parameter = list(support = 0.004, minlen = 2))
plot(rules, method = "graph", engine = "htmlwidget")
# Visualizacion de los resultados
inspect(sort(rules, by = 'support')[1:10])
setwd("~/repos/machinelearning-az/datasets/Part 5 - Association Rule Learning")
# Importar los datos
library(arules) # Instalar con install.packages('arules')
dataset = read.csv('data/Market_Basket_Optimisation.csv', header = FALSE)
dataset = read.transactions('data/Market_Basket_Optimisation.csv',
sep = ',', rm.duplicates = TRUE)
summary(dataset)
itemFrequencyPlot(dataset, topN = 100)
# Entrenar algoritmo Apriori con el dataset
rules = apriori(data = dataset,
parameter = list(support = 0.004, confidence = 0.2))
# Visualizacion de los resultados
inspect(sort(rules, by = 'lift')[1:10])
# Representacion grafica de las reglas de asociacion
library(arulesViz)
plot(rules, method = "graph", engine = "htmlwidget")
setwd("~/repos/machinelearning-az/datasets/Part 6 - Reinforcement Learning")
setwd("~/repos/machinelearning-az/datasets/Part 6 - Reinforcement Learning")
# Importar el dataset
dataset = read.csv('data/Ads_CTR_Optimisation.csv')
View(dataset)
View(dataset)
# Importing the dataset
dataset = read.csv('data/Ads_CTR_Optimisation.csv')
# Implementing Random Selection
N = 10000
d = 10
ads_selected = integer(0)
total_reward = 0
for (n in 1:N) {
ad = sample(1:10, 1)
ads_selected = append(ads_selected, ad)
reward = dataset[n, ad]
total_reward = total_reward + reward
}
# Visualising the results
hist(ads_selected,
col = 'blue',
main = 'Histogram of ads selections',
xlab = 'Ads',
ylab = 'Number of times each ad was selected')
# Upper Confidence Bound
setwd("~/repos/machinelearning-az/datasets/Part 6 - Reinforcement Learning")
# Importar el dataset
dataset = read.csv('data/Ads_CTR_Optimisation.csv')
# Implementar UCB
d = 10
N = 10000
number_of_selections = integer(d)
sums_of_rewards = integer(d)
ads_selected = integer(0)
total_reward = 0
for(n in 1:N){
max_upper_bound = 0
ad = 0
for(i in 1:d){
if(number_of_selections[i]>0){
average_reward = sums_of_rewards[i]/number_of_selections[i]
delta_i = sqrt(3/2*log(n)/number_selections[i])
upper_bound = average_reward + delta_i
}
else{
upper_bound = 1e400
}
if(upper_bound > max_upper_bound){
max_upper_bound = upper_bound
ad = i
}
}
ads_selected = append(ads_selected, ad)
number_of_selections[ad] = number_of_selections[ad] + 1
reward = dataset[n, ad]
sums_of_rewards[ad] = sums_of_rewards[ad] + reward
total_reward = total_reward + reward
}
# Upper Confidence Bound
setwd("~/repos/machinelearning-az/datasets/Part 6 - Reinforcement Learning")
# Importar el dataset
dataset = read.csv('data/Ads_CTR_Optimisation.csv')
# Implementar UCB
d = 10
N = 10000
number_of_selections = integer(d)
sums_of_rewards = integer(d)
ads_selected = integer(0)
total_reward = 0
for(n in 1:N){
max_upper_bound = 0
ad = 0
for(i in 1:d){
if(number_of_selections[i]>0){
average_reward = sums_of_rewards[i]/number_of_selections[i]
delta_i = sqrt(3/2*log(n)/number_of_selections[i])
upper_bound = average_reward + delta_i
}
else{
upper_bound = 1e400
}
if(upper_bound > max_upper_bound){
max_upper_bound = upper_bound
ad = i
}
}
ads_selected = append(ads_selected, ad)
number_of_selections[ad] = number_of_selections[ad] + 1
reward = dataset[n, ad]
sums_of_rewards[ad] = sums_of_rewards[ad] + reward
total_reward = total_reward + reward
}
ads_selected
hist(ads_selected,
col = 'blue',
main = 'Histograma de anuncios',
xlab = 'ID del Anuncio',
ylab = 'Frecuencia de visualizacion del anuncio')
# Visualizar los resultados - Histograma
hist(ads_selected,
col = 'lightblue',
main = 'Histograma de anuncios',
xlab = 'ID del Anuncio',
ylab = 'Frecuencia absoluta del anuncio')
# Muestreo de Thompson
setwd("~/repos/machinelearning-az/datasets/Part 6 - Reinforcement Learning")
# Importar el dataset
dataset = read.csv('data/Ads_CTR_Optimisation.csv')
# Implementar el Muestreo de Thompson
d = 10
N = 10000
number_of_rewards_1 = integer(d)
number_of_rewards_0 = integer(d)
ads_selected = integer(0)
total_reward = 0
for(n in 1:N){
max_random = 0
ad = 0
for(i in 1:d){
random_beta = rbeta(n = 1,
shape1 = number_of_rewards_1[i] + 1,
shape2 = number_of_rewards_0[i] + 1)
if(random_beta > max_random){
max_random = random_beta
ad = i
}
}
ads_selected = append(ads_selected, ad)
reward = dataset[n, ad]
if(reward == 1){
number_of_rewards_1[ad] = number_of_rewards_1[ad] + 1
}
else{
number_of_rewards_0[ad] = number_of_rewards_0[ad] + 1
}
total_reward = total_reward + reward
}
# Visualizar los resultados - Histograma
hist(ads_selected,
col = 'lightblue',
main = 'Histograma de anuncios',
xlab = 'ID del Anuncio',
ylab = 'Frecuencia absoluta del anuncio')
setwd("~/repos/machinelearning-az/datasets/Part 7 - Natural Language Processing")
# Natural Language Processing
setwd("~/repos/machinelearning-az/datasets/Part 7 - Natural Language Processing")
# Importar el dataset
dataset = read.csv('data/Restaurant_Reviews.tsv')
View(dataset)
dataset = read.csv('data/Restaurant_Reviews.tsv', sep = '\t')
View(dataset)
dataset = read.csv('data/Restaurant_Reviews.tsv', sep = '\t', quote = 3)
dataset = read.delim('data/Restaurant_Reviews.tsv')
# Natural Language Processing
setwd("~/repos/machinelearning-az/datasets/Part 7 - Natural Language Processing")
# Importar el dataset
dataset = read.delim('data/Restaurant_Reviews.tsv')
# Importar el dataset
dataset = read.delim('data/Restaurant_Reviews.tsv', quote = '')
# Importar el dataset
dataset = read.delim('data/Restaurant_Reviews.tsv', quote = '',
stringsAsFactors = FALSE)
install.packages('tm')
# Limpieza de textos
library(tm) # Instalar con install.packages('tm')
corpus = VCorpus(VectorSource(dataset$Review))
View(corpus)
View(dataset)
corpus = tm_map(corpus, content_transformer(tolower)) # Convertir a minuscula
corpus[0]
corpus[1]
corpus[[1]]
as.character(corpus[[1]])
as.character(corpus[[841]])
# Consultar el primer elemento del corpus
#as.character(corpus[[1]])
corpus = tm_map(corpus, removeNumbers)
as.character(corpus[[841]])
corpus = tm_map(corpus, removePunctuation) # Eliminar signos de puntuacion
as.character(corpus[[1]])
install.packages('SnallballC')
install.packages('RTools')
install.packages('SnowballC')
library(SnowballC) # Instalar con install.packages('SnowballC')
corpus = tm_map(corpus, removeWords, stopwords(kind = )) # Eliminar palabras irrelevantes
as.character(corpus[[1]])
install.packages('SnowballC')
install.packages("SnowballC")
install.packages("SnowballC")
install.packages("SnowballC")
detach("package:SnowballC", unload = TRUE)
install.packages("SnowballC")
install.packages("SnowballC")
install.packages("SnowballC")
# Natural Language Processing
setwd("~/repos/machinelearning-az/datasets/Part 7 - Natural Language Processing")
# Importar el dataset
dataset = read.delim('data/Restaurant_Reviews.tsv', quote = '',
stringsAsFactors = FALSE)
# Limpieza de textos
library(tm) # Instalar con install.packages('tm')
library(SnowballC) # Instalar con install.packages('SnowballC')
corpus = VCorpus(VectorSource(dataset$Review))
corpus = tm_map(corpus, content_transformer(tolower)) # Convertir a minuscula
# Consultar el primer elemento del corpus
#as.character(corpus[[1]])
corpus = tm_map(corpus, removeNumbers) # Eliminar numeros de las reviews del corpus
corpus = tm_map(corpus, removePunctuation) # Eliminar signos de puntuacion
corpus = tm_map(corpus, removeWords, stopwords(kind = 'en')) # Eliminar palabras irrelevantes
corpus = tm_map(corpus, stemDocument)
as.character(corpus[[1]])
corpus = tm_map(corpus, stripWhitespace) # Eliminar espacios en blanco
# Crear el modelo Bag of Words
dtm = DocumentTermMatrix(corpus)
View(dtm)
dtm
dtm = removeSparseTerms(dtm, 0.99)
dtm
dtm = removeSparseTerms(dtm, 0.999)
# Crear el modelo Bag of Words
dtm = DocumentTermMatrix(corpus)
dtm = removeSparseTerms(dtm, 0.999)
dtm
View(dataset)
dataset = as.data.frame(as.matrix(dtm))
View(dataset)
# Dividir los datos en conjunto de entrenamiento y conjunto de test
library(caTools) # Instalar con install.packages('caTools')
set.seed(123) # Para que salgan los mismos resultados que en el curso
split = sample.split(dataset$Purchased, SplitRatio = 0.75)
training_set = subset(dataset, split == TRUE)
testing_set = subset(dataset, split == FALSE)
# Escalado de valores
# Hay que indicar las columnas donde hacer el escalado
training_set[, 1:2] = scale(training_set[, 1:2])
testing_set[, 1:2] = scale(testing_set[, 1:2])
# Ajustar el clasificador de Random Forest con el conjunto de entrenamiento
library(randomForest) # Instalar con install.packages('randomForest')
classifier = randomForest(x = training_set[, -3],
y = training_set[, 3],
ntree = 10)
# Prediccion de los resultados con el conjunto de Testing
y_pred = predict(classifier, newdata = testing_set[, -3]) # Todo el dataset excluyendo la columna 3
# Crear la matriz de confusion
cm = table(testing_set[, 3], y_pred)
# Natural Language Processing
setwd("~/repos/machinelearning-az/datasets/Part 7 - Natural Language Processing")
# Importar el dataset
dataset_original = read.delim('data/Restaurant_Reviews.tsv', quote = '',
stringsAsFactors = FALSE)
# Limpieza de textos
library(tm) # Instalar con install.packages('tm')
library(SnowballC) # Instalar con install.packages('SnowballC')
corpus = VCorpus(VectorSource(dataset_original$Review))
corpus = tm_map(corpus, content_transformer(tolower)) # Convertir a minuscula
# Consultar el primer elemento del corpus
#as.character(corpus[[1]])
corpus = tm_map(corpus, removeNumbers) # Eliminar numeros de las reviews del corpus
corpus = tm_map(corpus, removePunctuation) # Eliminar signos de puntuacion
corpus = tm_map(corpus, removeWords, stopwords(kind = 'en')) # Eliminar palabras irrelevantes
corpus = tm_map(corpus, stemDocument) # Eliminar las "variantes" de palabras por su "raiz" ej: loved -> love
corpus = tm_map(corpus, stripWhitespace) # Eliminar espacios en blanco
# Crear el modelo Bag of Words
dtm = DocumentTermMatrix(corpus)
dtm = removeSparseTerms(dtm, 0.999)
dataset = as.data.frame(as.matrix(dtm))
dataset$Liked = dataset_original$Liked
dataset$Purchased = factor(dataset$Liked, levels = c(0, 1))
library(caTools) # Instalar con install.packages('caTools')
set.seed(123) # Para que salgan los mismos resultados que en el curso
split = sample.split(dataset$Purchased, SplitRatio = 0.80)
training_set = subset(dataset, split == TRUE)
testing_set = subset(dataset, split == FALSE)
# Natural Language Processing
setwd("~/repos/machinelearning-az/datasets/Part 7 - Natural Language Processing")
# Importar el dataset
dataset_original = read.delim('data/Restaurant_Reviews.tsv', quote = '',
stringsAsFactors = FALSE)
# Limpieza de textos
library(tm) # Instalar con install.packages('tm')
library(SnowballC) # Instalar con install.packages('SnowballC')
corpus = VCorpus(VectorSource(dataset_original$Review))
corpus = tm_map(corpus, content_transformer(tolower)) # Convertir a minuscula
# Consultar el primer elemento del corpus
#as.character(corpus[[1]])
corpus = tm_map(corpus, removeNumbers) # Eliminar numeros de las reviews del corpus
corpus = tm_map(corpus, removePunctuation) # Eliminar signos de puntuacion
corpus = tm_map(corpus, removeWords, stopwords(kind = 'en')) # Eliminar palabras irrelevantes
corpus = tm_map(corpus, stemDocument) # Eliminar las "variantes" de palabras por su "raiz" ej: loved -> love
corpus = tm_map(corpus, stripWhitespace) # Eliminar espacios en blanco
# Crear el modelo Bag of Words
dtm = DocumentTermMatrix(corpus)
dtm = removeSparseTerms(dtm, 0.999)
dataset = as.data.frame(as.matrix(dtm))
dataset$Liked = dataset_original$Liked
# Codificar la variable de clasificacion como factor
dataset$Purchased = factor(dataset$Liked, levels = c(0, 1))
# Dividir los datos en conjunto de entrenamiento y conjunto de test
library(caTools) # Instalar con install.packages('caTools')
set.seed(123) # Para que salgan los mismos resultados que en el curso
split = sample.split(dataset$Liked, SplitRatio = 0.80)
training_set = subset(dataset, split == TRUE)
testing_set = subset(dataset, split == FALSE)
View(dataset)
# Natural Language Processing
setwd("~/repos/machinelearning-az/datasets/Part 7 - Natural Language Processing")
# Importar el dataset
dataset_original = read.delim('data/Restaurant_Reviews.tsv', quote = '',
stringsAsFactors = FALSE)
# Limpieza de textos
# install.packages("tm")
#install.packages("SnowballC")
library(tm)
library(SnowballC)
corpus = VCorpus(VectorSource(dataset_original$Review))
corpus = tm_map(corpus, content_transformer(tolower))
# Consultar el primer elemento del corpus
# as.character(corpus[[1]])
corpus = tm_map(corpus, removeNumbers)
corpus = tm_map(corpus, removePunctuation)
corpus = tm_map(corpus, removeWords, stopwords(kind = "en"))
corpus = tm_map(corpus, stemDocument)
corpus = tm_map(corpus, stripWhitespace)
# Crear el modelo Bag of Words
dtm = DocumentTermMatrix(corpus)
dtm = removeSparseTerms(dtm, 0.999)
dataset = as.data.frame(as.matrix(dtm))
dataset$Liked = dataset_original$Liked
# Codificar la variable de clasificación como factor
dataset$Liked = factor(dataset$Liked, levels = c(0,1))
# Dividir los datos en conjunto de entrenamiento y conjunto de test
# install.packages("caTools")
library(caTools)
set.seed(123)
split = sample.split(dataset$Liked, SplitRatio = 0.80)
training_set = subset(dataset, split == TRUE)
testing_set = subset(dataset, split == FALSE)
# Natural Language Processing
setwd("~/repos/machinelearning-az/datasets/Part 7 - Natural Language Processing")
# Importar el dataset
dataset_original = read.delim('data/Restaurant_Reviews.tsv', quote = '',
stringsAsFactors = FALSE)
# Limpieza de textos
library(tm) # Instalar con install.packages('tm')
library(SnowballC) # Instalar con install.packages('SnowballC')
corpus = VCorpus(VectorSource(dataset_original$Review))
corpus = tm_map(corpus, content_transformer(tolower)) # Convertir a minuscula
# Consultar el primer elemento del corpus
#as.character(corpus[[1]])
corpus = tm_map(corpus, removeNumbers) # Eliminar numeros de las reviews del corpus
corpus = tm_map(corpus, removePunctuation) # Eliminar signos de puntuacion
corpus = tm_map(corpus, removeWords, stopwords(kind = 'en')) # Eliminar palabras irrelevantes
corpus = tm_map(corpus, stemDocument) # Eliminar las "variantes" de palabras por su "raiz" ej: loved -> love
corpus = tm_map(corpus, stripWhitespace) # Eliminar espacios en blanco
# Crear el modelo Bag of Words
dtm = DocumentTermMatrix(corpus)
dtm = removeSparseTerms(dtm, 0.999)
dataset = as.data.frame(as.matrix(dtm))
dataset$Liked = dataset_original$Liked
# Codificar la variable de clasificacion como factor
dataset$Purchased = factor(dataset$Liked, levels = c(0, 1))
# Dividir los datos en conjunto de entrenamiento y conjunto de test
library(caTools) # Instalar con install.packages('caTools')
set.seed(123) # Para que salgan los mismos resultados que en el curso
split = sample.split(dataset$Liked, SplitRatio = 0.80)
training_set = subset(dataset, split == TRUE)
testing_set = subset(dataset, split == FALSE)
# Natural Language Processing
setwd("~/repos/machinelearning-az/datasets/Part 7 - Natural Language Processing")
# Importar el dataset
dataset_original = read.delim('data/Restaurant_Reviews.tsv', quote = '',
stringsAsFactors = FALSE)
# Limpieza de textos
library(tm) # Instalar con install.packages('tm')
library(SnowballC) # Instalar con install.packages('SnowballC')
corpus = VCorpus(VectorSource(dataset_original$Review))
corpus = tm_map(corpus, content_transformer(tolower)) # Convertir a minuscula
# Consultar el primer elemento del corpus
#as.character(corpus[[1]])
corpus = tm_map(corpus, removeNumbers) # Eliminar numeros de las reviews del corpus
corpus = tm_map(corpus, removePunctuation) # Eliminar signos de puntuacion
corpus = tm_map(corpus, removeWords, stopwords(kind = 'en')) # Eliminar palabras irrelevantes
corpus = tm_map(corpus, stemDocument) # Eliminar las "variantes" de palabras por su "raiz" ej: loved -> love
corpus = tm_map(corpus, stripWhitespace) # Eliminar espacios en blanco
# Crear el modelo Bag of Words
dtm = DocumentTermMatrix(corpus)
dtm = removeSparseTerms(dtm, 0.999)
dataset = as.data.frame(as.matrix(dtm))
dataset$Liked = dataset_original$Liked
# Codificar la variable de clasificacion como factor
dataset$Liked = factor(dataset$Liked, levels = c(0, 1))
# Dividir los datos en conjunto de entrenamiento y conjunto de test
library(caTools) # Instalar con install.packages('caTools')
set.seed(123) # Para que salgan los mismos resultados que en el curso
split = sample.split(dataset$Liked, SplitRatio = 0.80)
training_set = subset(dataset, split == TRUE)
testing_set = subset(dataset, split == FALSE)
# Ajustar el clasificador de Random Forest con el conjunto de entrenamiento
library(randomForest) # Instalar con install.packages('randomForest')
classifier = randomForest(x = training_set[, -692],
y = training_set$Liked,
ntree = 10)
# Prediccion de los resultados con el conjunto de Testing
y_pred = predict(classifier, newdata = testing_set[, -692]) # Todo el dataset excluyendo la columna 3
# Crear la matriz de confusion
cm = table(testing_set[692], y_pred)
# Crear la matriz de confusion
.
cm = table(testing_set[, 692], y_pred)
cm
y_pred = predict(classifier, newdata = 'hello')
View(testing_set)
